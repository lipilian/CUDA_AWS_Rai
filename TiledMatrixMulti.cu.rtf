{\rtf1\ansi\ansicpg1252\cocoartf2509
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\margl1440\margr1440\vieww10800\viewh8400\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs24 \cf0 #include <wb.h>\
\
#define wbCheck(stmt)                                                     \\\
  do \{                                                                    \\\
    cudaError_t err = stmt;                                               \\\
    if (err != cudaSuccess) \{                                             \\\
      wbLog(ERROR, "Failed to run stmt ", #stmt);                         \\\
      wbLog(ERROR, "Got CUDA error ...  ", cudaGetErrorString(err));      \\\
      return -1;                                                          \\\
    \}                                                                     \\\
  \} while (0)\
#define TILE_WIDTH 4\
// Compute C = A * B\
__global__ void matrixMultiplyShared(float *A, float *B, float *C,\
                                     int numARows, int numAColumns,\
                                     int numBRows, int numBColumns,\
                                     int numCRows, int numCColumns) \{\
  //@@ Insert code to implement matrix multiplication here\
  __shared__ float subTileM[TILE_WIDTH][TILE_WIDTH];\
  __shared__ float subTileN[TILE_WIDTH][TILE_WIDTH];\
  \
  int bx = blockIdx.x; int by = blockIdx.y;\
  int tx = threadIdx.x; int ty = threadIdx.y;\
  \
  int Row = by * TILE_WIDTH + ty;\
  int Col = bx * TILE_WIDTH + tx;\
  float Pvalue = 0.0;\
  \
  for (int m = 0; m < numAColumns/TILE_WIDTH; ++m) \{\
    subTileM[ty][tx] = A[Row * numAColumns + m * TILE_WIDTH + tx];\
    subTileN[ty][tx] = B[(m * TILE_WIDTH + ty) * numBColumns + Col];\
    __syncthreads();\
    for(int k = 0; k <TILE_WIDTH; ++k)\{\
      Pvalue += subTileM[ty][k] * subTileN[k][tx];\
    \}\
    __syncthreads();\
  \}\
  C[Row * numCColumns + Col] = Pvalue;\
  \
  //@@ You have to use shared memory for this MP\
\}\
\
int main(int argc, char **argv) \{\
  wbArg_t args;\
  float *hostA; // The A matrix\
  float *hostB; // The B matrix\
  float *hostC; // The output C matrix\
  float *deviceA;\
  float *deviceB;\
  float *deviceC;\
  int numARows;    // number of rows in the matrix A\
  int numAColumns; // number of columns in the matrix A\
  int numBRows;    // number of rows in the matrix B\
  int numBColumns; // number of columns in the matrix B\
  int numCRows;    // number of rows in the matrix C (you have to set this)\
  int numCColumns; // number of columns in the matrix C (you have to set\
                   // this)\
\
  args = wbArg_read(argc, argv);\
\
  wbTime_start(Generic, "Importing data and creating memory on host");\
  hostA = (float *)wbImport(wbArg_getInputFile(args, 0), &numARows,\
                            &numAColumns);\
  hostB = (float *)wbImport(wbArg_getInputFile(args, 1), &numBRows,\
                            &numBColumns);\
  //@@ Set numCRows and numCColumns\
  numCRows = numARows;\
  numCColumns = numBColumns;\
  //@@ Allocate the hostC matrix\
  size_t sizeA = numARows * numAColumns * sizeof(float);\
  size_t sizeB = numBRows * numBColumns * sizeof(float);\
  size_t sizeC = numCRows * numCColumns * sizeof(float);\
  hostC = (float *)malloc(sizeC);\
  wbTime_stop(Generic, "Importing data and creating memory on host");\
\
  wbLog(TRACE, "The dimensions of A are ", numARows, " x ", numAColumns);\
  wbLog(TRACE, "The dimensions of B are ", numBRows, " x ", numBColumns);\
\
  wbTime_start(GPU, "Allocating GPU memory.");\
  //@@ Allocate GPU memory here\
  cudaMalloc((void **)&deviceA, sizeA);\
  cudaMalloc((void **)&deviceB, sizeB);\
  cudaMalloc((void **)&deviceC, sizeC);\
  wbTime_stop(GPU, "Allocating GPU memory.");\
\
  wbTime_start(GPU, "Copying input memory to the GPU.");\
  //@@ Copy memory to the GPU here\
  cudaMemcpy(deviceA, hostA, sizeA, cudaMemcpyHostToDevice);\
  cudaMemcpy(deviceB, hostB, sizeB, cudaMemcpyHostToDevice);\
  cudaMemcpy(deviceC, hostC, sizeC, cudaMemcpyHostToDevice);\
  wbTime_stop(GPU, "Copying input memory to the GPU.");\
\
  //@@ Initialize the grid and block dimensions here\
  dim3 dimGrid(numCColumns/4,numCRows/4, 1);\
  dim3 dimBlock(4,4,1);\
  wbTime_start(Compute, "Performing CUDA computation");\
  //@@ Launch the GPU Kernel here\
  matrixMultiplyShared<<<dimGrid, dimBlock>>>(deviceA, deviceB, deviceC, numARows,\
                               numAColumns, numBRows,\
                               numBColumns, numCRows,\
                               numCColumns);\
  cudaDeviceSynchronize();\
  wbTime_stop(Compute, "Performing CUDA computation");\
\
  wbTime_start(Copy, "Copying output memory to the CPU");\
  //@@ Copy the GPU memory back to the CPU here\
  cudaMemcpy(hostC, deviceC, sizeC, cudaMemcpyDeviceToHost);\
  wbTime_stop(Copy, "Copying output memory to the CPU");\
\
  wbTime_start(GPU, "Freeing GPU Memory");\
  //@@ Free the GPU memory here\
  cudaFree(deviceA);\
  cudaFree(deviceB);\
  cudaFree(deviceC);\
  wbTime_stop(GPU, "Freeing GPU Memory");\
\
  wbSolution(args, hostC, numCRows, numCColumns);\
\
  free(hostA);\
  free(hostB);\
  free(hostC);\
\
  return 0;\
\}}